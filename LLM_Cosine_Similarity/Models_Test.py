from sentence_transformers import SentenceTransformer, util
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel
import warnings
import torch
warnings.filterwarnings('ignore')


models = {
    "alibaba": {
        "model": SentenceTransformer('Alibaba-NLP/gte-multilingual-base', trust_remote_code=True),
        "type": "sentence"
    },

    "e5": {
        "model": AutoModel.from_pretrained('intfloat/multilingual-e5-base'),
        "tokenizer": AutoTokenizer.from_pretrained('intfloat/multilingual-e5-base'),
        "prefix": "query: ",
        "type": "transformer"
    },

    "e5_small": {
        "model": AutoModel.from_pretrained('intfloat/multilingual-e5-small'),
        "tokenizer": AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small'),
        "prefix": "query: ",
        "type": "transformer"
    },

    "distiluse_v1": {
        "model": SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1'),
        "type": "sentence"
    },

    "nomic_ai": {
        "model": AutoModel.from_pretrained("nomic-ai/nomic-embed-text-v2-moe", trust_remote_code=True),
        "tokenizer": AutoTokenizer.from_pretrained("nomic-ai/nomic-embed-text-v2-moe"),
        "type": "transformer"
    },

    "labse": {
        "model": SentenceTransformer('sentence-transformers/LaBSE'),
        "type": "sentence"
    },

    "trendyol": {
        "model": SentenceTransformer("Trendyol/TY-ecomm-embed-multilingual-base-v1.2.0", trust_remote_code=True, truncate_dim=768),
        "type": "sentence"
    },
    
    "snowflake": {
        "model": SentenceTransformer('Snowflake/snowflake-arctic-embed-l-v2.0'),
        "type": "sentence_with_prompt"
    }
}

alibaba_model = models["alibaba"]["model"]
e5_model = models["e5"]["model"]
e5_tokenizer = models["e5"]["tokenizer"]
e5_small_model = models["e5_small"]["model"]
e5_small_tokenizer = models["e5_small"]["tokenizer"]
distiluse_model = models["distiluse_v1"]["model"]
nomic_ai_model = models["nomic_ai"]["model"]
nomic_ai_tokenizer = models["nomic_ai"]["tokenizer"]
labse_model = models["labse"]["model"]
trendyol_model = models["trendyol"]["model"]
snowflake_model = models["snowflake"]["model"]

"""
test_words = [     
    # TÃ¼rkÃ§e-Ä°ngilizce Ã‡eviriler (YÃ¼ksek similarity beklenir)
    ("elma", "apple"),
    ("kÃ¶pek", "dog"),
    ("araba", "car"),
    ("ev", "house"),
    ("kitap", "book"),
    ("government", "hÃ¼kÃ¼met"),
    ("school", "okul"),
    ("water", "su"),
    
    # Benzer Anlamlar (Orta similarity beklenir)
    ("kÃ¶pek", "kedi"),
    ("bÃ¼yÃ¼k", "dev"),
    ("hÄ±zlÄ±", "sÃ¼ratli"),
    ("gÃ¼zel", "hoÅŸ"),
    
    # ZÄ±t Anlamlar (DÃ¼ÅŸÃ¼k similarity beklenir)
    ("bÃ¼yÃ¼k", "kÃ¼Ã§Ã¼k"),
    ("mutlu", "Ã¼zgÃ¼n"),
    ("sÄ±cak", "soÄŸuk"),
    ("gece", "gÃ¼ndÃ¼z"),
    
    # Ä°lgisiz Kelimeler (Ã‡ok dÃ¼ÅŸÃ¼k similarity beklenir)
    ("elma", "bilgisayar"),
    ("kÃ¶pek", "matematik"),
    ("araba", "ÅŸarkÄ±"),
    ("government tax", "silah")
]

"""

test_words = [
    # TÃ¼rkÃ§e-Ä°ngilizce EÅŸdeÄŸer Kurumsal Terimler (YÃ¼ksek similarity beklenir)
    ("mali rapor", "financial report"),
    ("vergi", "tax"),
    ("fatura", "invoice"),
    ("sÃ¶zleÅŸme", "contract"),
    ("ÅŸirket", "company"),
    ("yÃ¶netim kurulu", "board of directors"),
    ("hisse", "stock"),
    ("bilanÃ§o", "balance sheet"),
    ("muhasebe", "accounting"),
    ("bÃ¼tÃ§e", "budget"),
    ("nakit akÄ±ÅŸÄ±", "cash flow"),
    ("karlÄ±lÄ±k", "profitability"),
    ("satÄ±ÅŸ", "sales"),
    ("pazarlama", "marketing"),
    ("insan kaynaklarÄ±", "human resources"),
    ("iÅŸe alÄ±m", "recruitment"),
    ("maaÅŸ bordrosu", "payroll"),
    ("performans deÄŸerlendirmesi", "performance evaluation"),
    ("operasyon", "operation"),
    ("proje yÃ¶netimi", "project management"),
    ("risk analizi", "risk analysis"),
    ("iÅŸ planÄ±", "business plan"),
    ("strateji", "strategy"),
    ("yatÄ±rÄ±m", "investment"),

    # Benzer Anlamlar (Orta similarity beklenir)
    ("fatura", "Ã¶deme belgesi"),
    ("hisse", "pay"),
    ("yÃ¶netici", "mÃ¼dÃ¼r"),
    ("gider", "harcama"),
    ("karlÄ±lÄ±k", "getiri"),
    ("personel", "Ã§alÄ±ÅŸan"),
    ("mÃ¼ÅŸteri", "istemci"),
    ("satÄ±ÅŸ", "ticaret"),
    ("toplantÄ±", "gÃ¶rÃ¼ÅŸme"),
    ("rapor", "dokÃ¼man"),
    ("analiz", "inceleme"),
    ("planlama", "organizasyon"),

    # ZÄ±t Anlamlar (DÃ¼ÅŸÃ¼k similarity beklenir)
    ("gelir", "gider"),
    ("kÃ¢r", "zarar"),
    ("aktif", "pasif"),
    ("alacak", "borÃ§"),
    ("satÄ±ÅŸ", "satÄ±n alma"),
    ("artÄ±ÅŸ", "azalÄ±ÅŸ"),
    ("baÅŸarÄ±", "baÅŸarÄ±sÄ±zlÄ±k"),
    ("verimlilik", "verimsizlik"),
    ("geniÅŸleme", "kÃ¼Ã§Ã¼lme"),
    ("yatÄ±rÄ±m", "geri Ã§ekme"),
    ("yÃ¼kselme", "dÃ¼ÅŸÃ¼ÅŸ"),
    ("kazanÃ§", "kayÄ±p"),

    # Ä°lgisiz Kelimeler (Ã‡ok dÃ¼ÅŸÃ¼k similarity beklenir)
    ("bilanÃ§o", "Ã§alÄ±ÅŸan motivasyonu"),
    ("vergi", "ÅŸirket logosu"),
    ("sÃ¶zleÅŸme", "mÃ¼zik listesi"),
    ("yÃ¶netim kurulu", "hava durumu raporu"),
    ("muhasebe", "spor mÃ¼sabakasÄ±"),
    ("mali rapor", "yemek tarifi"),
    ("nakit akÄ±ÅŸÄ±", "film eleÅŸtirisi"),
    ("karlÄ±lÄ±k", "bahÃ§e tasarÄ±mÄ±"),
    ("proje yÃ¶netimi", "moda trendi"),
    ("risk analizi", "seyahat rehberi"),
    ("performans deÄŸerlendirmesi", "mÃ¼ze koleksiyonu"),
    ("insan kaynaklarÄ±", "uzay araÅŸtÄ±rmasÄ±")
]

def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)
    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]

def test_sentence_transformer_model(model, word1, word2):
    emb1 = model.encode(word1, convert_to_tensor=True)
    emb2 = model.encode(word2, convert_to_tensor=True)
    return util.pytorch_cos_sim(emb1, emb2).item()

def test_snowflake_model(model, word1, word2):
    # Snowflake modeli iÃ§in Ã¶zel test fonksiyonu
    query_emb = model.encode([word1], prompt_name="query", convert_to_tensor=True)
    doc_emb = model.encode([word2], convert_to_tensor=True)
    return util.pytorch_cos_sim(query_emb, doc_emb).item()

def test_transformers_model(model, tokenizer, word1, word2, prefix=""):
    texts = [f"{prefix}{word1}", f"{prefix}{word2}"]
    batch = tokenizer(texts, max_length=512, padding=True, truncation=True, return_tensors='pt')
    
    with torch.no_grad():
        outputs = model(**batch)
    
    embeddings = average_pool(outputs.last_hidden_state, batch['attention_mask'])
    embeddings = F.normalize(embeddings, p=2, dim=1)
    
    return (embeddings[0] @ embeddings[1].T).item()

def run_comprehensive_test():
    print("KAPSAMLI MODEL TEST SÄ°STEMÄ°")
    print("=" * 80)
    
    all_results = []
    model_totals = { model_name: [] for model_name in models.keys() }
    model_wins = { model_name: 0 for model_name in models.keys() }

    for i, (word1, word2) in enumerate(test_words, 1):
        print(f"\n Test {i:2d}/{len(test_words)}: '{word1}' â†” '{word2}'")
        print("-" * 60)
        
        pair_results = {'word1': word1, 'word2': word2}
        
        try:
            similarity = test_sentence_transformer_model(alibaba_model, word1, word2)
            pair_results['alibaba'] = similarity
            model_totals['alibaba'].append(similarity)
            print(f"  {'Alibaba GTE':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'Alibaba GTE':>15}: HATA - {str(e)[:28]}...")
            pair_results['alibaba'] = None

        try:
            similarity = test_transformers_model(e5_model, e5_tokenizer, word1, word2, "query: ")
            pair_results['e5'] = similarity
            model_totals['e5'].append(similarity)
            print(f"  {'E5 Multilingual':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'E5 Multilingual':>15}: HATA - {str(e)[:28]}...")
            pair_results['e5'] = None

        try:
            similarity = test_transformers_model(e5_small_model, e5_small_tokenizer, word1, word2, "query: ")
            pair_results['e5_small'] = similarity
            model_totals['e5_small'].append(similarity)
            print(f"  {'E5 Small':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'E5 Small':>15}: HATA - {str(e)[:28]}...")
            pair_results['e5_small'] = None

        try:
            similarity = test_sentence_transformer_model(distiluse_model, word1, word2)
            pair_results['distiluse_v1'] = similarity
            model_totals['distiluse_v1'].append(similarity)
            print(f"  {'DistilUSE v1':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'DistilUSE v1':>15}: HATA - {str(e)[:28]}...")
            pair_results['distiluse_v1'] = None

        try:
            similarity = test_transformers_model(nomic_ai_model, nomic_ai_tokenizer, word1, word2)
            pair_results['nomic_ai'] = similarity
            model_totals['nomic_ai'].append(similarity)
            print(f"  {'Nomic AI':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'Nomic AI':>15}: HATA - {str(e)[:28]}...")
            pair_results['nomic_ai'] = None

        try:
            similarity = test_sentence_transformer_model(labse_model, word1, word2)
            pair_results['labse'] = similarity
            model_totals['labse'].append(similarity)
            print(f"  {'LaBSE':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'LaBSE':>15}: HATA - {str(e)[:28]}...")
            pair_results['labse'] = None

        try:
            similarity = test_sentence_transformer_model(trendyol_model, word1, word2)
            pair_results['trendyol'] = similarity
            model_totals['trendyol'].append(similarity)
            print(f"  {'Trendyol':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'Trendyol':>15}: HATA - {str(e)[:28]}...")
            pair_results['trendyol'] = None

        try:
            similarity = test_snowflake_model(snowflake_model, word1, word2)
            pair_results['snowflake'] = similarity
            model_totals['snowflake'].append(similarity)
            print(f"  {'Snowflake Arctic':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'Snowflake Arctic':>15}: HATA - {str(e)[:28]}...")
            pair_results['snowflake'] = None

        valid_scores = {model: score for model, score in pair_results.items() if model not in ['word1', 'word2'] and score is not None}

        # Sadece ilk 24 test (TÃ¼rkÃ§e-Ä°ngilizce Ã§eviriler) iÃ§in kazananÄ± say
        if valid_scores and i <= 24:  # i <= 24 Ã§Ã¼nkÃ¼ ilk 24 test TÃ¼rkÃ§e-Ä°ngilizce Ã§eviriler
            best_model = max(valid_scores.items(), key=lambda x: x[1])[0]
            model_wins[best_model] += 1

        all_results.append(pair_results)
    
    print("\n" + "=" * 80)
    print("ğŸ“Š GENEL SONUÃ‡LAR VE ANALÄ°Z")
    print("=" * 80)
    
    print("\nğŸ† MODEL ORTALAMA PERFORMANSLARI:")
    print("-" * 50)
    model_averages = {}
    for model_name, scores in model_totals.items():
        if scores:
            avg = sum(scores) / len(scores)
            model_averages[model_name] = avg
            print(f"  {model_name:>15}: {avg:.4f} ({len(scores)} test)")
        else:
            print(f"  {model_name:>15}: Test edilemedi")
    

    print(f"\nğŸ“ˆ KATEGORÄ° ANALÄ°ZÄ°:")
    print("-" * 30)
    
    categories = {
        'TÃ¼rkÃ§e-Ä°ngilizce Ã‡eviriler': test_words[0:24],
        'Benzer Anlamlar': test_words[24:36],
        'ZÄ±t Anlamlar': test_words[36:48],
        'Ä°lgisiz Kelimeler': test_words[48:60]
    }
    
    for cat_name, cat_words in categories.items():
        print(f"\n  {cat_name}:")
        
        category_scores = {}
        for model_name in model_averages.keys():
            cat_scores = []
            for word1, word2 in cat_words:
                for result in all_results:
                    if result['word1'] == word1 and result['word2'] == word2:
                        if result[model_name] is not None:
                            cat_scores.append(result[model_name])
            
            if cat_scores:
                cat_avg = sum(cat_scores) / len(cat_scores)
                category_scores[model_name] = cat_avg
        
        sorted_category = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)
        for model_name, score in sorted_category:
            print(f"    {model_name:>15}: {score:.4f}")
    
    print(f"\nğŸ¥Š TÃœRKÃ‡E-Ä°NGÄ°LÄ°ZCE Ã‡EVÄ°RÄ° BAÅARI SONUÃ‡LARI:")
    print("-" * 45)
    sorted_wins = sorted(model_wins.items(), key=lambda x: x[1], reverse=True)
    for i, (model, score) in enumerate(sorted_wins, 1):
        print(f"  {i}. {model}: {score}/24 test kazandÄ±")

    return all_results, model_averages


if __name__ == "__main__":
    print("ğŸ”§ Modeller yÃ¼kleniyor...")
    print("Bu iÅŸlem biraz zaman alabilir...")
    print("\n")
    
    results, averages = run_comprehensive_test()
    
    print(f"\nâœ… Test tamamlandÄ±!")
    print(f"ğŸ“Š {len(test_words)} kelime Ã§ifti test edildi")
    print(f"ğŸ¤– {len(models)} model karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±")


