from sentence_transformers import SentenceTransformer, util
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel
import warnings
import torch
warnings.filterwarnings('ignore')


models = {
    "alibaba": {
        "model": SentenceTransformer('Alibaba-NLP/gte-multilingual-base', trust_remote_code=True),
        "type": "sentence"
    },

    "e5": {
        "model": AutoModel.from_pretrained('intfloat/multilingual-e5-base'),
        "tokenizer": AutoTokenizer.from_pretrained('intfloat/multilingual-e5-base'),
        "prefix": "query: ",
        "type": "transformer"
    },

    "e5_small": {
        "model": AutoModel.from_pretrained('intfloat/multilingual-e5-small'),
        "tokenizer": AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small'),
        "prefix": "query: ",
        "type": "transformer"
    },

    "distiluse_v1": {
        "model": SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1'),
        "type": "sentence"
    },

    "nomic_ai": {
        "model": AutoModel.from_pretrained("nomic-ai/nomic-embed-text-v2-moe", trust_remote_code=True),
        "tokenizer": AutoTokenizer.from_pretrained("nomic-ai/nomic-embed-text-v2-moe"),
        "type": "transformer"
    },

    "labse": {
        "model": SentenceTransformer('sentence-transformers/LaBSE'),
        "type": "sentence"
    },

    "trendyol": {
        "model": SentenceTransformer("Trendyol/TY-ecomm-embed-multilingual-base-v1.2.0", trust_remote_code=True, truncate_dim=768),
        "type": "sentence"
    },
    
    "snowflake": {
        "model": SentenceTransformer('Snowflake/snowflake-arctic-embed-l-v2.0'),
        "type": "sentence_with_prompt"
    }
}

alibaba_model = models["alibaba"]["model"]
e5_model = models["e5"]["model"]
e5_tokenizer = models["e5"]["tokenizer"]
e5_small_model = models["e5_small"]["model"]
e5_small_tokenizer = models["e5_small"]["tokenizer"]
distiluse_model = models["distiluse_v1"]["model"]
nomic_ai_model = models["nomic_ai"]["model"]
nomic_ai_tokenizer = models["nomic_ai"]["tokenizer"]
labse_model = models["labse"]["model"]
trendyol_model = models["trendyol"]["model"]
snowflake_model = models["snowflake"]["model"]

"""
test_words = [     
    # TÃ¼rkÃ§e-Ä°ngilizce Ã‡eviriler (YÃ¼ksek similarity beklenir)
    ("elma", "apple"),
    ("kÃ¶pek", "dog"),
    ("araba", "car"),
    ("ev", "house"),
    ("kitap", "book"),
    ("government", "hÃ¼kÃ¼met"),
    ("school", "okul"),
    ("water", "su"),
    
    # Benzer Anlamlar (Orta similarity beklenir)
    ("kÃ¶pek", "kedi"),
    ("bÃ¼yÃ¼k", "dev"),
    ("hÄ±zlÄ±", "sÃ¼ratli"),
    ("gÃ¼zel", "hoÅŸ"),
    
    # ZÄ±t Anlamlar (DÃ¼ÅŸÃ¼k similarity beklenir)
    ("bÃ¼yÃ¼k", "kÃ¼Ã§Ã¼k"),
    ("mutlu", "Ã¼zgÃ¼n"),
    ("sÄ±cak", "soÄŸuk"),
    ("gece", "gÃ¼ndÃ¼z"),
    
    # Ä°lgisiz Kelimeler (Ã‡ok dÃ¼ÅŸÃ¼k similarity beklenir)
    ("elma", "bilgisayar"),
    ("kÃ¶pek", "matematik"),
    ("araba", "ÅŸarkÄ±"),
    ("government tax", "silah")
]

"""

test_words = [
    # TÃ¼rkÃ§e-Ä°ngilizce EÅŸdeÄŸer Kurumsal Terimler (YÃ¼ksek similarity beklenir)
    ("mali rapor", "financial report"),
    ("vergi", "tax"),
    ("fatura", "invoice"),
    ("sÃ¶zleÅŸme", "contract"),
    ("ÅŸirket", "company"),
    ("yÃ¶netim kurulu", "board of directors"),
    ("hisse", "stock"),
    ("bilanÃ§o", "balance sheet"),

    # Benzer Anlamlar (Orta similarity beklenir)
    ("fatura", "Ã¶deme belgesi"),
    ("hisse", "pay"),
    ("yÃ¶netici", "mÃ¼dÃ¼r"),
    ("gider", "harcama"),

    # ZÄ±t Anlamlar (DÃ¼ÅŸÃ¼k similarity beklenir)
    ("gelir", "gider"),
    ("kÃ¢r", "zarar"),
    ("aktif", "pasif"),
    ("alacak", "borÃ§"),

    # Ä°lgisiz Kelimeler (Ã‡ok dÃ¼ÅŸÃ¼k similarity beklenir)
    ("bilanÃ§o", "Ã§alÄ±ÅŸan motivasyonu"),
    ("vergi", "ÅŸirket logosu"),
    ("sÃ¶zleÅŸme", "mÃ¼zik listesi"),
    ("yÃ¶netim kurulu", "hava durumu raporu")
]

def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)
    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]

def test_sentence_transformer_model(model, word1, word2):
    emb1 = model.encode(word1, convert_to_tensor=True)
    emb2 = model.encode(word2, convert_to_tensor=True)
    return util.pytorch_cos_sim(emb1, emb2).item()

def test_snowflake_model(model, word1, word2):
    # Snowflake modeli iÃ§in Ã¶zel test fonksiyonu
    query_emb = model.encode([word1], prompt_name="query", convert_to_tensor=True)
    doc_emb = model.encode([word2], convert_to_tensor=True)
    return util.pytorch_cos_sim(query_emb, doc_emb).item()

def test_transformers_model(model, tokenizer, word1, word2, prefix=""):
    texts = [f"{prefix}{word1}", f"{prefix}{word2}"]
    batch = tokenizer(texts, max_length=512, padding=True, truncation=True, return_tensors='pt')
    
    with torch.no_grad():
        outputs = model(**batch)
    
    embeddings = average_pool(outputs.last_hidden_state, batch['attention_mask'])
    embeddings = F.normalize(embeddings, p=2, dim=1)
    
    return (embeddings[0] @ embeddings[1].T).item()

def run_comprehensive_test():
    print("KAPSAMLI MODEL TEST SÄ°STEMÄ°")
    print("=" * 80)
    
    all_results = []
    model_totals = { model_name: [] for model_name in models.keys() }
    
    for i, (word1, word2) in enumerate(test_words, 1):
        print(f"\n Test {i:2d}/{len(test_words)}: '{word1}' â†” '{word2}'")
        print("-" * 60)
        
        pair_results = {'word1': word1, 'word2': word2}
        
        try:
            similarity = test_sentence_transformer_model(alibaba_model, word1, word2)
            pair_results['alibaba'] = similarity
            model_totals['alibaba'].append(similarity)
            print(f"  {'Alibaba GTE':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'Alibaba GTE':>15}: HATA - {str(e)[:20]}...")
            pair_results['alibaba'] = None

        try:
            similarity = test_transformers_model(e5_model, e5_tokenizer, word1, word2, "query: ")
            pair_results['e5'] = similarity
            model_totals['e5'].append(similarity)
            print(f"  {'E5 Multilingual':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'E5 Multilingual':>15}: HATA - {str(e)[:20]}...")
            pair_results['e5'] = None

        try:
            similarity = test_transformers_model(e5_small_model, e5_small_tokenizer, word1, word2, "query: ")
            pair_results['e5_small'] = similarity
            model_totals['e5_small'].append(similarity)
            print(f"  {'E5 Small':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'E5 Small':>15}: HATA - {str(e)[:20]}...")
            pair_results['e5_small'] = None

        try:
            similarity = test_sentence_transformer_model(distiluse_model, word1, word2)
            pair_results['distiluse_v1'] = similarity
            model_totals['distiluse_v1'].append(similarity)
            print(f"  {'DistilUSE v1':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'DistilUSE v1':>15}: HATA - {str(e)[:20]}...")
            pair_results['distiluse_v1'] = None

        try:
            similarity = test_transformers_model(nomic_ai_model, nomic_ai_tokenizer, word1, word2)
            pair_results['nomic_ai'] = similarity
            model_totals['nomic_ai'].append(similarity)
            print(f"  {'Nomic AI':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'Nomic AI':>15}: HATA - {str(e)[:20]}...")
            pair_results['nomic_ai'] = None

        try:
            similarity = test_sentence_transformer_model(labse_model, word1, word2)
            pair_results['labse'] = similarity
            model_totals['labse'].append(similarity)
            print(f"  {'LaBSE':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'LaBSE':>15}: HATA - {str(e)[:20]}...")
            pair_results['labse'] = None

        try:
            similarity = test_sentence_transformer_model(trendyol_model, word1, word2)
            pair_results['trendyol'] = similarity
            model_totals['trendyol'].append(similarity)
            print(f"  {'Trendyol':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'Trendyol':>15}: HATA - {str(e)[:20]}...")
            pair_results['trendyol'] = None

        try:
            similarity = test_snowflake_model(snowflake_model, word1, word2)
            pair_results['snowflake'] = similarity
            model_totals['snowflake'].append(similarity)
            print(f"  {'Snowflake Arctic':>15}: {similarity:.4f}")
        except Exception as e:
            print(f"  {'Snowflake Arctic':>15}: HATA - {str(e)[:20]}...")
            pair_results['snowflake'] = None

        all_results.append(pair_results)
    
    print("\n" + "=" * 80)
    print("ğŸ“Š GENEL SONUÃ‡LAR VE ANALÄ°Z")
    print("=" * 80)
    
    print("\nğŸ† MODEL ORTALAMA PERFORMANSLARI:")
    print("-" * 50)
    model_averages = {}
    for model_name, scores in model_totals.items():
        if scores:
            avg = sum(scores) / len(scores)
            model_averages[model_name] = avg
            print(f"  {model_name:>15}: {avg:.4f} ({len(scores)} test)")
        else:
            print(f"  {model_name:>15}: Test edilemedi")
    
    if model_averages:
        print(f"\nğŸ¥‡ PERFORMANS SIRALAMASI:")
        print("-" * 30)
        sorted_models = sorted(model_averages.items(), key=lambda x: x[1], reverse=True)
        for i, (model, score) in enumerate(sorted_models, 1):
            print(f"  {i}. {model}: {score:.4f}")
    
    print(f"\nğŸ“ˆ KATEGORÄ° ANALÄ°ZÄ°:")
    print("-" * 30)
    
    categories = {
        'TÃ¼rkÃ§e-Ä°ngilizce Ã‡eviriler': test_words[0:8],
        'Benzer Anlamlar': test_words[8:12],
        'ZÄ±t Anlamlar': test_words[12:16],
        'Ä°lgisiz Kelimeler': test_words[16:20]
    }
    
    for cat_name, cat_words in categories.items():
        print(f"\n  {cat_name}:")
        
        # Her kategori iÃ§in model performanslarÄ±nÄ± hesapla
        category_scores = {}
        for model_name in model_averages.keys():
            cat_scores = []
            for word1, word2 in cat_words:
                for result in all_results:
                    if result['word1'] == word1 and result['word2'] == word2:
                        if result[model_name] is not None:
                            cat_scores.append(result[model_name])
            
            if cat_scores:
                cat_avg = sum(cat_scores) / len(cat_scores)
                category_scores[model_name] = cat_avg
        
        # BÃ¼yÃ¼kten kÃ¼Ã§Ã¼ÄŸe sÄ±rala ve yazdÄ±r
        sorted_category = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)
        for model_name, score in sorted_category:
            print(f"    {model_name:>15}: {score:.4f}")
    
    print(f"\nğŸ¯ EN YÃœKSEK SÄ°MÄ°LARÄ°TY SKORLARI:")
    print("-" * 40)
    all_scores = []
    for result in all_results:
        for model_name, score in result.items():
            if model_name not in ['word1', 'word2'] and score is not None:
                all_scores.append((f"{result['word1']}-{result['word2']}", model_name, score))
    
    all_scores.sort(key=lambda x: x[2], reverse=True)
    for pair, model, score in all_scores[:5]:
        print(f"  {pair:>25} - {model}: {score:.4f}")
    
    print(f"\nğŸ“‰ EN DÃœÅÃœK SÄ°MÄ°LARÄ°TY SKORLARI:")
    print("-" * 40)
    for pair, model, score in all_scores[-5:]:
        print(f"  {pair:>25} - {model}: {score:.4f}")
    
    return all_results, model_averages

if __name__ == "__main__":
    print("ğŸ”§ Modeller yÃ¼kleniyor...")
    print("Bu iÅŸlem biraz zaman alabilir...")
    print("\n")
    
    results, averages = run_comprehensive_test()
    
    print(f"\nâœ… Test tamamlandÄ±!")
    print(f"ğŸ“Š {len(test_words)} kelime Ã§ifti test edildi")
    print(f"ğŸ¤– {len(models)} model karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±")


